/**
 * Test script to verify AI integration with proper environment loading
 */

import dotenv from 'dotenv'
import path from 'path'

// Load environment variables from .env.local
dotenv.config({ path: path.join(process.cwd(), '.env.local') })

import { OpenAIService } from '../services/llm/openai-service'
import type { UserContext } from '../types/engine'

async function testAIIntegration() {
  console.log('ğŸ¤– Testing AI Integration with Environment Variables...')
  console.log('=' .repeat(55))

  // Debug environment variables
  console.log('ğŸ” Environment Check:')
  console.log('  OPENAI_API_KEY:', process.env.OPENAI_API_KEY ? 'âœ… Found' : 'âŒ Not found')
  console.log('  Working Directory:', process.cwd())
  console.log('')

  const llmService = new OpenAIService()
  
  console.log('ğŸš€ LLM Service Status:', llmService.isReady() ? 'âœ… Ready' : 'âš ï¸  Fallback Mode')
  console.log('')
  
  // Mock user context for testing
  const mockUserContext: UserContext = {
    userId: 'test_user',
    sessionId: 'test_session',
    name: 'Alex',
    currentStep: 'completed',
    timeContext: {
      timeOfDay: 'evening',
      dayOfWeek: 'Monday',
      timezone: 'Asia/Shanghai'
    },
    emotionalState: {
      mood: 'excited',
      energy: 'high',
      engagement: 'high'
    },
    preferences: {
      communicationStyle: 'casual',
      creativityLevel: 'experimental',
      contentTypes: ['text', 'image'],
      topics: ['technology', 'creativity', 'art']
    },
    recentInteractions: [],
    capabilities: [],
    goals: []
  }

  // Mock capabilities
  const mockCapabilities = [
    {
      id: 'gpt4-text',
      name: 'GPT-4 Text Generation',
      type: 'model' as const,
      description: 'Advanced text generation model',
      capabilities: ['text_generation', 'creative_writing', 'storytelling'],
      metadata: { provider: 'openai', version: '4.0' }
    },
    {
      id: 'dalle3-image',
      name: 'DALL-E 3 Image Generation',
      type: 'model' as const,
      description: 'AI image generation model',
      capabilities: ['image_generation', 'visual_art'],
      metadata: { provider: 'openai', version: '3.0' }
    }
  ]

  // Test 1: Dynamic Scenario Generation
  console.log('ğŸ­ Test 1: AI-Powered Dynamic Scenario Generation')
  console.log('-'.repeat(45))
  
  try {
    const scenario = await llmService.generateDynamicScenario(mockUserContext, mockCapabilities)
    console.log('âœ… Generated Scenario:')
    console.log('  ğŸ“ Title:', scenario.title)
    console.log('  ğŸ“„ Description:', scenario.description)
    console.log('  ğŸ’¬ Prompt:', scenario.prompt)
    console.log('  ğŸ·ï¸  Tags:', scenario.tags.join(', '))
    console.log('')
  } catch (error) {
    console.log('âŒ Scenario generation failed:', error)
    console.log('')
  }

  // Test 2: Intent Processing
  console.log('ğŸ§  Test 2: AI-Powered Intent Processing')
  console.log('-'.repeat(35))
  
  const testInputs = [
    "æˆ‘æƒ³åˆ›ä½œä¸€ä¸ªå…³äºæœºå™¨äººå­¦ä¹ ç»˜ç”»çš„ç§‘å¹»æ•…äº‹",
    "Help me design a logo for my startup",
    "I need to write a professional email to my boss"
  ]
  
  for (const input of testInputs) {
    try {
      console.log(`ğŸ“ Input: "${input}"`)
      const intent = await llmService.processUserIntent(input, mockUserContext)
      console.log('  ğŸ¯ Primary Goal:', intent.primaryGoal)
      console.log('  ğŸ“Š Content Type:', intent.contentType)
      console.log('  ğŸ˜Š Emotional Tone:', intent.emotionalTone)
      console.log('  âš¡ Urgency:', intent.urgency)
      console.log('  ğŸ¯ Confidence:', (intent.confidence * 100).toFixed(0) + '%')
      console.log('')
    } catch (error) {
      console.log('  âŒ Intent processing failed:', error)
      console.log('')
    }
  }

  // Test 3: Capability Explanation
  console.log('ğŸ’¡ Test 3: AI-Powered Capability Explanation')
  console.log('-'.repeat(40))
  
  for (const capability of mockCapabilities) {
    try {
      console.log(`ğŸ”§ Explaining: ${capability.name}`)
      const explanation = await llmService.explainCapability(capability, mockUserContext)
      console.log('  ğŸ’¬ Explanation:', explanation)
      console.log('')
    } catch (error) {
      console.log('  âŒ Capability explanation failed:', error)
      console.log('')
    }
  }

  console.log('ğŸ‰ AI Integration Test Complete!')
  
  if (llmService.isReady()) {
    console.log('')
    console.log('ğŸš€ AI Features Status: ACTIVE')
    console.log('  âœ… Dynamic scenario generation with GPT-4')
    console.log('  âœ… Intelligent intent processing')
    console.log('  âœ… Personalized capability explanations')
    console.log('  âœ… Context-aware responses')
    console.log('')
    console.log('ğŸ¯ Next Steps:')
    console.log('  1. ğŸ–¥ï¸  Frontend integration with React')
    console.log('  2. ğŸ”„ Real-time WebSocket communication')
    console.log('  3. âš¡ Execution and Output layers')
    console.log('  4. ğŸ¨ Multi-modal content generation')
  } else {
    console.log('')
    console.log('âš ï¸  Running in Fallback Mode')
    console.log('ğŸ’¡ To enable AI features:')
    console.log('  1. Ensure .env.local exists in project root')
    console.log('  2. Add: OPENAI_API_KEY=your_actual_api_key')
    console.log('  3. Restart this test')
  }
}

// Run the test
testAIIntegration().catch(console.error)
